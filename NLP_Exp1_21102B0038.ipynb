{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cfe1a40-06d6-4b6e-b856-e32a6fdd1bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from autocorrect import Speller\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a911257f-2a8c-421f-89c6-92b414aff59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autocorrect\n",
      "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
      "     ---------------------------------------- 0.0/622.8 kB ? eta -:--:--\n",
      "     - ---------------------------------- 30.7/622.8 kB 435.7 kB/s eta 0:00:02\n",
      "     ----------------- ------------------- 297.0/622.8 kB 3.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- 622.8/622.8 kB 3.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: autocorrect\n",
      "  Building wheel for autocorrect (setup.py): started\n",
      "  Building wheel for autocorrect (setup.py): finished with status 'done'\n",
      "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622375 sha256=13bf28041b15bffb5ca4e7e2555be1cc9fcd40e08a48536bdf6f9712b4c32da9\n",
      "  Stored in directory: c:\\users\\deep salunkhe\\appdata\\local\\pip\\cache\\wheels\\5e\\90\\99\\807a5ad861ce5d22c3c299a11df8cba9f31524f23ae6e645cb\n",
      "Successfully built autocorrect\n",
      "Installing collected packages: autocorrect\n",
      "Successfully installed autocorrect-2.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install autocorrect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3ddab4f-7240-4220-becf-7d9b6283b623",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Deep\n",
      "[nltk_data]     Salunkhe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Deep\n",
      "[nltk_data]     Salunkhe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Deep Salunkhe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\Deep\n",
      "[nltk_data]     Salunkhe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to C:\\Users\\Deep\n",
      "[nltk_data]     Salunkhe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Deep\n",
      "[nltk_data]     Salunkhe\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')  # For tokenization\n",
    "nltk.download('stopwords')  # For stop word removal\n",
    "nltk.download('averaged_perceptron_tagger')  # For POS tagging\n",
    "nltk.download('maxent_ne_chunker')  # For Named Entity Recognition\n",
    "nltk.download('words')  # Required for NER\n",
    "nltk.download('wordnet')  # For lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbfc4c95-784e-41b2-af2d-2b1069942e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(corpus):\n",
    "    # Sentence segmentation: Split the corpus into individual sentences\n",
    "    sentences = sent_tokenize(corpus)\n",
    "    \n",
    "    processed_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Lowercasing: Convert all text to lowercase\n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        # Number handling: Remove all digits\n",
    "        # Note: This simple approach removes all numbers. Modify as needed.\n",
    "        sentence = re.sub(r'\\d+', '', sentence)\n",
    "        \n",
    "        # Tokenization: Split sentence into individual words\n",
    "        tokens = word_tokenize(sentence)\n",
    "        \n",
    "        # Stop word removal: Remove common words that don't carry much meaning\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # Spelling correction: Correct misspelled words\n",
    "        # Note: This uses the autocorrect library and may need adjustments\n",
    "        spell = Speller()\n",
    "        tokens = [spell(token) for token in tokens]\n",
    "        \n",
    "        # Text normalization: Lemmatization\n",
    "        # Convert words to their base or dictionary form\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        # Named Entity Recognition (NER)\n",
    "        # First, perform Part-of-Speech (POS) tagging\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        # Then, identify named entities\n",
    "        named_entities = ne_chunk(pos_tags)\n",
    "        \n",
    "        # Reconstruct the sentence from processed tokens\n",
    "        processed_sentence = ' '.join(tokens)\n",
    "        processed_sentences.append(processed_sentence)\n",
    "    \n",
    "    # Join all processed sentences back into a single text\n",
    "    processed_corpus = ' '.join(processed_sentences)\n",
    "    \n",
    "    # Return both the processed corpus and the named entities\n",
    "    return processed_corpus, named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85856d6f-7917-4a38-9fcb-31a64972af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"\n",
    "Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\n",
    "\"\"\"\n",
    "\n",
    "# Apply preprocessing to the example corpus\n",
    "processed_corpus, named_entities = preprocess_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95510aa7-e961-475d-87a1-25278e890b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Corpus:\n",
      "lore ipsum simply dummy text printing typesetting industry . lore ipsum industry 's standard dummy text ever since , unknown printer took galley type scrambled make type specimen book . survived five century , also leap electronic typesetting , remaining essentially unchanged . popularised release letraset sheet containing lore ipsum passage , recently desktop publishing software like album pacemaker including version lore ipsum .\n",
      "\n",
      "Named Entities:\n",
      "(S\n",
      "  popularised/JJ\n",
      "  release/NN\n",
      "  letraset/JJ\n",
      "  sheet/NN\n",
      "  containing/VBG\n",
      "  lore/JJR\n",
      "  ipsum/JJ\n",
      "  passage/NN\n",
      "  ,/,\n",
      "  recently/RB\n",
      "  desktop/VBD\n",
      "  publishing/NN\n",
      "  software/NN\n",
      "  like/IN\n",
      "  album/NN\n",
      "  pacemaker/NN\n",
      "  including/VBG\n",
      "  version/NN\n",
      "  lore/RB\n",
      "  ipsum/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(\"Processed Corpus:\")\n",
    "print(processed_corpus)\n",
    "\n",
    "print(\"\\nNamed Entities:\")\n",
    "print(named_entities)\n",
    "\n",
    "# Note: The output will show the processed text with lowercased words,\n",
    "# removed stop words and numbers, corrected spellings, and lemmatized words.\n",
    "# Named entities will be displayed separately in a tree structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7eda95-a852-4d22-b47a-3225d70113e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
